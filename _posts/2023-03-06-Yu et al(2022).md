---
layout: post
title: Width & Depth Pruning for Vision Transformers 리뷰
category: 논문
tag: [transformer, ViT, Pruning]
---

## Abstract

트랜스포머 모델은 컴퓨터 비전 도메인에서 높은 성능과 포텐셜을 보여줬지만 대량의 계산을 필요로 하는 특성 상, 엣지 디바이스에서 운용하기 어려웠다고 합니다. 최근 연구는 ViT(Vision Transformer)에서 덜 중요한 unit을 제거하는데 집중하고 있습니다. 그러나 ViT에서 중요한 depth는 무시한채로 네트워크의 width만 고려하였고, 이에 본 연구진은 Width & Depth Pruning (WDPruning) 프레임워크를 제시합니다. width pruning에서는 학습할 수 있는  pruning-related 파라미터 세트를 이용하여 트랜스포머 모델의 width를 수정합니다. 반면, depth pruning에서는 트랩스포머 블록간의 중간 정보를 전달할 수 있는 shallow classifier를 생성하여 깊은 classifier를 대체합니다. 학습할 때는 shallow classifier가 필요하지만 모델을 인퍼런스할 때는 shallow classifier가 필요없습니다. 벤치마크 데이터셋을 활용한 실험을 통해 대표 ViT 모델인 DeiT와 Swin Transformer 모델에서 마이너한 정확도 하락으로 컴퓨팅 계산량을 줄이는 데 성공했습니다. 특히, ILSVRC-12에서는 심지어 0.14%의 정확도 향상과 함께 22%의 pruning ratio를 달성하는데 성공했다.

## Introduction

트랜스포머 아키텍쳐는 본래 자연어처리 (NLP)에서 많이 사용되어 좋은 성과를 걷어왔었다. 근래에 들어서, 본 논문에서 다루게 될 Vision Transformer (ViT) 연구 트렌드는 아래와 같습니다.

- Dosovitskiy et al. 2020 : Vision Transformer 시초
- Jiang et al. 2021; Meng 2021; Hassani et al. 2021 : 이미지 분류에서 SOTA 결과를 달성
- Carion et al. 2020; Zhu et al. 2020 : 객체 인식에서 SOTA 결과를 달성
- Wang et al. 2021 : 인스턴스 segmentation애서 SOTA 결과를 달성

그러나 트랜스포머는 엄청나게 많은 연산량을 요구하므로 엣지 디바이스에서 inference하는데 한계가 있었습니다. 이에, 트랜스포머 압축을 통해 효율을 향상하고자 하는 여러 노력이 있었습니다. 가장 대표적으로 시도된 연구는 아래와 같습니다.

- Han et al. 2015 : Weight Pruning

weight pruing은 현저한 모델 사이즈 감량을 할 수 있지만 경량화 결과가 unstructured sparse matrix(구조화되지 않는 희소행렬)에 제시되면서 일반적인 하드웨어에 적용하기 어려웠습니다. 이에, 덜 중요한 row나 column을 통째로 없애버리는 structured pruning이 제시되고 있습니다. 이러한 맥락에서 아래와 같은 연구들이 제시되어 왔습니다.

- Goyal et al. 2020 (PoWER) : NLP forward pass에서 word token을 제거하여 BERT 모델(Devlin et al. 2018)을 가속화시킴
- Yao et al. 2021 (MLPruning) : BERT-related model을 pruning하는 multi-level pruning 구조를 제시함
- Michael et al. 2019 : BERT 모델의 self-attetion을 pruning하여 약간의 성능 개선을 확인함

그러나 ViT에서의 여러 변종(Liu et al. 2021a, Yuan et al. 2021)들이 탁월한 성능을 내기 시작했고 여기에서는 기존의 pruning 기법을 사용할 수 없었습니다. 본 연구에서 주목한 모델은 VTP (Zhu et al. 2021)입니다. VTP 모델의 자세한 내용은 아래와 같습니다.

- VTP는 Liu et al. 2017의 network slimming을 확장하여 training, pruning and fine-tuning 시 linear matrix의 차원을 줄였습니다.
- 특히, VTP에서는 soft mask score에서 structured sparsity regularization을 도입하여 학습해야 하는데 (???)
- 학습 뒤에는 VTP는 매뉴얼하게 고른 threshold보다 작은 soft mask score를 폐기한 뒤, fine-tuning합니다.

기존의 모델들은 network width만 고려하였지만 network depth를 고려하지 않았습니다. 실험을 통해 본 연구헤서는 동일한 pruning rate에서 width pruning보다 depth pruning이 더 좋은 parallelism efficiency를 달성할 수 있음을 알았습니다. 이는 syncronization과 fragmentation이더 적기 때문이라고 주장하고 있습니다.

본 연구에서는 width와 depth를 fine-tuning 없이 동시에 pruning할 수 있는 프레임워크를 제시합니다. width를 줄이기 위해서 본 연구에서는 학습 가능한 saliency score 와 score threshold를 사용하였습니다. 사용자가 설정한 pruning ratio를 지키기 위해 학습을 하는 동안 Augmented Lagrangian method를 사용하여 score threshold는 각각의 층의 pruning ratio를 다이나믹하게 업데이트합니다. depth를 줄이기 위해서 본 연구에서는 트랜스포머 블록의 중간 정보를 사용하여 몇몇 얇은 분류기를 도입하였습니다. 이는 깊은 분류기가 대신에 얇은 분류기로 이미지를 분류할 수 있게 합니다. inference 시에는 얇은 분류기 다음의 모든 트랜스포머 블록은 drop됩니다. 본 연구의 contribution은 아래와 같습니다.

- width pruning : 본 연구에서는 학습 가능한 saliency score와 층별 threshold를 사용하여 층간 sparsity가 다를 수 있는 모델을 제시했습니다.
- depth pruning : 본 연구에서는 트랜스포머 블록 뒤에 plug-in classifier를 도입하여 한 번의 학습 과정에서 순차적으로 블록을 삭제할 수 있으며, 네트워크 성능과 pruning rate간의 균형을 쉽게 제어할 수 있습니다.
- performance : 광범위한 실험을 통해 효과와 효율성을 입증하였습니다. 처리량을 15% 개선하면서 정확도 하락은 1% 미만으로 하였습니다.

## Related Works

## Methodology

Formulation

