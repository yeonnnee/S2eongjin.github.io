---
layout: post
title: 강화학습 시리즈1_의사결정을 위한 MDP 기초, 정책, 가치함수
category: 강화학습
tag: RL
---

## MDP 기초

강화학습의 가장 기본이 되는 두 가지 요소는 에이전트와 환경입니다. 그리고 그 둘은 서로 행동과 상태, 보상을 주고 받습니다. 이를 도식화한 그림은 아래와 같습니다.

<a href="https://imgur.com/63OlhGq"><img src="https://imgur.com/63OlhGq.png" width="700px" title="source: imgur.com" /></a>

강화학습은 시퀀셜하게 연속되는 스텝의 반복입니다. 아래 항목은 그 하나의 스텝을 나타냅니다.

매 시점 $t$마다,
- 에이전트는 현재 상태 $s_t$를 관측
- 에이전트는 $s_t$를 이용하여 적합한 행동 $a_t$를 구하여 환경에 인가
- 환경은 행동 $a_t$를 반영
- 환경은 행동 $a_t$에 의해 변경된 다음 상태 $s_{t+1}$ 및 보상 $r_{t+1}$를 에이전트에 인가

강화학습을 처음 접하신 분들은 새로 듣게 되는 단어가 많으실 것 같은데요, 앞서 언급된 각 요소에 대해서 설명드리도록 하겠습니다.

- 에이전트: 관측과 행동의 주체
- 환경: 해결해야 하는 문제 혹은 그 문제가 일어나는 공간
- 상태: 환경의 현재 상황을 잘 표현하는 관측값
- 행동: 에이전트가 환경의 현재 상황을 보고 추론한 다음 상태의 행동
- 보상: 하나의 숫자로 표현된 행동에 대한 평가지표

위 요소는 강화학습이라는 도메인에서 가장 중요한 개념입니다. 하지만 딥러닝이나 다른 도메인에 계셨던 분들은 처음 들었을 거라 생소할 것이라 생각하는데요, 이 개념들은 시리즈에서 계속 언급되니 지금 눈에 들어오지 않더라도 크게 걱정하실 필요는 없습니다.

## 마르코프 특성

"어떤 문제가 마르코프하다"라는 말을 들어보신 적 있으신가요? 어떤 시퀀스의 조건부 확률밀도함수가 아래와 같으면 마르코프 시퀀스라고 합니다.

$$
P(s_{t+1}|s_t, s_{t-1}, ..., s_0, a_t, a_{t-1}, ..., a_0) = P(s_{t+1}|s_t, a_t)
$$

식을 풀이하자면 과거의 상태와 과거의 행동은 필요 없이 현재의 상태와 현재의 행동 정보만 있으면 다음 상태를 알 수 있다는 말입니다. 이해가 어려우신 분들을 위해 5가지 예시를 통해 설명드리겠습니다.

질문: 다음 상황은 마르코프한가요?
1. 자동차를 운전한다.
2. 주어진 주식에 투자할 것인지 결정한다.
3. 축구 경기에서 어느 팀이 이길지 예측한다.
4. 어떤 목적지로의 최단 경로를 선택한다.
5. 평궁삼거리의 교통신호를 제어한다.

정답은 다음과 같습니다!
1. 마르코프합니다. 자동차를 운전하는 운전자는 핸들을 꺾꺼나 액셀/브레이크를 밟기 위해 과거 궤적을 알 필요가 없습니다.
2. 마르코프하지 않습니다. 과거의 시퀀셜한 데이터를 기반으로 미래의 주식 동향을 예측한 뒤 투자를 결정하기 때문입니다.
3. 마르코프하지 않습니다. 과거의 승률을 분석하여 예측하기 때문입니다.
4. 마르코프합니다. 최단 경로는 현재 주어진 도로의 geometry만 있으면 구할 수 있기 때문입니다.
5. 마르코프합니다. 현재 도로의 상태 정보만 있으면 제어할 수 있고, 그 도로가 한 시간 전에 막혔는지 안 막혔는지는 중요하지 않습니다.

## 마르코프 과정

다음으로 마르코프 과정에 대해 알아보겠습니다. 마르코프 과정은 상태 $S$와 상태 천이 행렬 $P$로 이루어진 튜플입니다.

<a href="https://imgur.com/oEBmsB5"><img src="https://imgur.com/oEBmsB5.png" width="300px" title="source: imgur.com" /></a>

$S$는 앞서 설명한대로 현재 상태입니다. $P$는 상태 천이 행렬인데 뒤에서 추가적으로 알아보겠습니다. 우선 $S$에 집중하겠습니다. 아래 그림과 같이 간단한 예시를 들어보겠습니다. 평궁삼거리의 교통신호를 강화학습으로 제어하기 위해서 평궁삼거리의 상태를 파악하고자 합니다.

<a href="https://imgur.com/mv2LBUa"><img src="https://imgur.com/mv2LBUa.png" width="300px" title="source: imgur.com" />

이 때, 3개 진입로의 정체 여부를 각각 소통 원활(O)과 정체(X)로 나누면 평궁삼거리의 상태는 총 8개로 나타낼 수 있습니다. 

<a href="https://imgur.com/1sF0p1e"><img src="https://imgur.com/1sF0p1e.png" width="300px" title="source: imgur.com" /></a>

현재 3개 진입로가 모두 정체 상태(X)라고 해보겠습니다. 아래 그림에서 정체 상태(X)는 빨간색으로 표시하고 소통 원활 상태(O)는 초록색으로 표시하겠습니다. 다음 상태는 오른쪽 그림과 같이 총 8가지 가짓수가 가능합니다.

<a href="https://imgur.com/uN5y0UZ"><img src="https://imgur.com/uN5y0UZ.png" width="700px" title="source: imgur.com" /></a>

이 때, 아래 그림에서 각 상태로 변화할 확률을 생각해보겠습니다. 다음 상태가 되었는데도 계속 막힐 확률이 40프로나 되네요. 나머지 방향이 각각 한 군데 뚫리는 확률은 20프로로 산정되는 것을 볼 수 있습니다. 이 그림은 현재 상태가 모든 방향이 막힌 상황인 경우만 표현한 것이고, 실제 상황은 8개의 모든 상황 모두 현재 상태가 될 수 있겠습니다. 따라서 저런 화살표는 무려 8의제곱 64개가 필요합니다. 자기 스스로도 가리키는 화살표가 있기 때문입니다.

<a href="https://imgur.com/XOfzlZS"><img src="https://imgur.com/XOfzlZS.png" width="700px" title="source: imgur.com" /></a>

## 마르코프 연쇄

다만 변환될 확률이 0인 화살표를 제외하고 그리는 것이 일반적이고 이것이 바로 마르코프 연쇄입니다.

<a href="https://imgur.com/6eQnzOi"><img src="https://imgur.com/6eQnzOi.png" width="700px" title="source: imgur.com" /></a>

## 상태 천이 행렬

마르코프 연쇄 모형에서 도출했던 각 변화 확률은 행렬로 변환할 수 있습니다. 세로열에는 현재 상태를 나열하고, 가로행에는 다음 상태를 나열한뒤, 천이될 확률을 채워넣으면 됩니다. 현재 특정 상태에서 다음 특정 상태로 이동할 확률의 모음을 상태 천이 행렬이라고 합니다. 현재 상태 $s$에서 다음 상태 $s'$으로 이동할 확률 $P_{ss'}$은 아래와 같이 정의합니다.

- $$P_{ss'} \equiv P(S_{t+1}=s'|S_t=s)$$

<a href="https://imgur.com/JvARw83"><img src="https://imgur.com/JvARw83.png" width="700px" title="source: imgur.com" /></a>

## 행동

평궁삼거리 예시를 계속 가져와보겠습니다. 평궁삼거리에서 에이전트는 신호제어기입니다. 신호제어기는 현재 신호 주기($t$)의 상태($s_t$)를 관찰하여 행동($a_t$)한 뒤 다음 주기($t+1$)의 상태($s_{t+1}$)를 얻습니다. 신호제어기가 행동할 수 있는 선택지는 아래 그림과 같이 5가지입니다. 직진과 좌회전의 시간 배분을 달리한 5개의 조합입니다. 다만, 이는 이해를 돕기 위해 하나의 예시를 설명한 것이고, 실제는 더욱 효율적인 행동을 할 수 있습니다.

<a href="https://imgur.com/DLsuuPO"><img src="https://imgur.com/DLsuuPO.png" width="700px" title="source: imgur.com" /></a>

## 마르코프 결정 과정

마르코프 연쇄와 행동을 합치면 비로소 마르코프 결정 과정(Markov Decision Process, MDP)가 됩니다.

- 마르코프 결정 과정(MDP): $<S, A, P, R, \gamma>$인 튜플
  - $S$는 (유한한) 상태의 집합
  - $A$는 (유한한) 행동의 집합
  - $$P$$는 상태 천이 행렬, $$P_{SS'}^a = P[S_{t+1}=s'|S_t = s, A_t = a]$$
  - $R$는 보상 함수, $R:S\times A\rightarrow R$ (하나의 숫자)
  - $\gamma$는 감소율, $\gamma\in[0, 1]$ (0 이상 1 이하의 모든 실수 중 하나)

<a href="https://imgur.com/fJOUzAk"><img src="https://imgur.com/fJOUzAk.png" width="700px" title="source: imgur.com" /></a>

위 그림에서 누워있는 각각의 표는 상태 천이 행렬입니다. 그런데 그 행렬이 여러 개 있습니다. 각 행동별로 상태 천이 행렬이 다르게 정의되기 때문입니다. 따라서 마르코프 결정 과정, MDP는 3차원으로 정의됩니다. 처음 보는 2개 단어가 눈에 띌 겁니다. 보상과 감소율인데요.

## 보상

보상 개념에 대해 설명드리겠습니다. 혹자는 강화학습은 걸음마를 떼는 아기에 비유했습니다. 이는 강화학습의 핵심 컨셉인데요. 잘했을 때는 상을 주고, 못 했을 때는 벌을 줍니다. 행동을 했는데도 현재 막혀있는 상태에 대비해서 변함이 없는 1번의 경우, 벌을 줘야겠죠, 1점 감점입니다. 2번 경우와 6번 경우를 비교해보겠습니다. 아니 똑같은 직진 방향인데 왜 6번에 더 많은 보상을 주냐구요? 그 이유는 예를 들면 남쪽에서 진입하는 교통량이 더 많거나 교통량 중 버스의 비율이 높을 수 있겠습니다.

<a href="https://imgur.com/omCwmiK"><img src="https://imgur.com/omCwmiK.png" width="700px" title="source: imgur.com" /></a>

이번에는 서쪽 진입로만 괜찮은 상황이네요. 이때는 현상 유지하면 0점을 얻고 다시 막혀버리면 7점 감점을 합니다. 이래야 에이전트는 막히지 않도록 애를 쓰겠죠? 이렇게 방향별 교통류 별로 보상을 다르게 설계하는 것은 리워드 엔지니어링이라고 합니다. 강화학습은 기존 최적화 문제에서 복잡하게 다뤘던 변수들간의 관계를 내려놓고, 상태들로만 가져오기 때문에 보상 체계를 어떻게 하느냐가 중요합니다.

<a href="https://imgur.com/59wG3bS"><img src="https://imgur.com/59wG3bS.png" width="700px" title="source: imgur.com" /></a>

## 리턴

마지막 개념입니다! 매 스텝마다 에이전트는 현재 상태를 평가해야 하는데요. 이는 현재 상황으로부터 전체 미래에 받을 것으로 예상되는 전체 보상의 감가된 합계입니다. 여기에서 '감가'라는 개념이 생소하실 것 같은데요. 할인율의 비율만큼 감가된다고 보시면 될 것 같습니다. 만약 할인율이 1에 가까워지면 먼 미래의 보상까지 그대로 반영하겠다는 것이며, 할인율이 0에 가까워지면 미래의 보상보다 현재의 보상을 더욱 중요시하는 것입니다.

<a href="https://imgur.com/zFbP3QE"><img src="https://imgur.com/zFbP3QE.png" width="700px" title="source: imgur.com" /></a>